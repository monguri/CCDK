<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" href="../vce.css" type="text/css">
<title>VCE #VER# official document</title>
</head>

<body>
<h1>VCE ベンチマーク</h1>
<p>
  VCE を使うとどれぐらいの性能を出せるのか、
ベンチマークテストを通して考察する。
</p>

<h3>環境</h3>
<p>
  安価ですぐ交換可能なハードウェアといえば、各種 Ethernet である。
ここではまず最も一般的な 100Base-TX
と普通のスイッチを使った環境でテストをした。
この環境で ping -f テストを2個同時に走らせると、
データサイズが 64バイト以下では、約14000 pkt/s であった。
データサイズが 100バイトでは、 5100 pkt/s となり、
1000バイトでは 約1200 pkt/s であった。
</p>
<BR>
<p>
ping -f の両端のマシンはそれぞれ P3-1GHz*1, P3-933MHz*2 である。
限界速度時点でのCPU使用使用率はそれぞれ安定して 1% 以下であった。
<BR>
</p>
<BR>
<p>
このテストにより、ネットワークやマシンなど、
テスト環境自体には問題はないことがわかる。
</p>

<h3>テストに使うプログラム</h3>
<pre>
autotest/nogen_sv :  VEC を使わないテストサーバ(エコーバック)
autotest/nogen_vecsv :  VEC を使うテストサーバ(エコーバック)
autotest/nogen_cl :  クライアント(エコーを期待)
</pre>
<p>
  テストでは Q/s (クエリ毎秒) を計測する。
テストは以下の手順を踏む。
</p>
<ol>
  <LI>まずサーバを起動する
  <LI>そのサーバに対して複数のクライアントを接続する
  <LI>全部のクライアントが接続に成功し定常状態に入ったのを確認
  <LI>サーバ側でQ/sを数える
</ol>
<p>
サーバは受信したデータをそのままエコーするので、
今回のテストは、受信量と送信量の比が完全に1対1である。
またサーバは、受信したデータが正しい配列に並んでいるか、
バッファの内容が正しいかなどをチェックしていることに注意が必要である。
100Mbpsの通信時には、このチェックのために、メモリ帯域幅を 400Mbps 程度
食いつぶすが、これは限界速度の数%を占めるからである。
</p>
<p>
テスト結果には、クエリサイズ、クライアント数、
VEC の有無、暗号化の有無が大きな影響を与えるのでそれ毎に設定を変えて計測する。
クライアント数は、10個以下で大きな変化がみられ、
10以上だとだんだん変化がゆるやかになる。
</p>

<h3>一般的な現象</h3>
<p>
1本のセッションを使って、限界速度でデータを転送するような場合は、
「(long) fat pipe」での通信となるので、
サーバ、クライアント側の双方に十分なバッファサイズを用意することが必要である。
そうでない場合は帯域を活かしきることはできない。
100Mbps で1本の場合は、1Mbyte 以上のバッファを用意すると、
スムーズにデータを転送できる。
</p>

<h3>接続方法</h3>
<p>
  約1000クライアントまでは、カーネルの設定に特に手を加えなくても、
クライアント用マシン1台対サーバ用マシン1台で実験できる。
そのときには
</p>
<pre>
limit openfiles 1024
</pre>
<p>
といったコマンドをシェルから入力する。
</p>


<h3>非暗号化、中継なしの場合</h3>
<p>
  最もライトなコンテンツでは、 VEC も使わず暗号も使わない。
以下の接続ではどの部分でも暗号は使っていない。
</p>
<pre>
        192.168.1.0/24
 Server --- Switch ---- Client
 P3 933        |         P3 1G
               +------- Client ... (コネクションが多い場合)

</pre>
<p>
以下では、右端に delayed あるいは nodelay と記している。
一般に NODELAY があると Q/s は低下するがレスポンスは向上する。
クエリサイズが小さい場合その違いが顕著である。
クライアント数が増えたりクエリがでかいと差は縮まる。
</p>
<font color=red>k Q/s</font> という記述は、 1000クエリ毎秒の略である。
<Pre>
1byte, 1 client : 21.9k Q/s (nodelay)
1byte, 1 client : 50.1k Q/s (delayed)
1byte, 16 client : 31.5k Q/s (nodelay)
1byte, 16 client : 36.6k Q/s (delayed)
1byte, 512 client : 19.9k Q/s (nodelay)
32byte, 1 client : 21.4k Q/s (nodelay)
32byte, 512 client : 19.5k Q/s (nodelay)
128byte, 128 client : 20.8k Q/s (delayed)
256byte, 16 client : 19.1k Q/s  (nodelay)
256byte, 16 client : 29.0k Q/s (delayed)
256byte, 128 client : 15.4k Q/s (delayed)
256byte, 256 client : 15.1k Q/s (delayed)
256byte, 512 client : 13.3k Q/s (delayed)
512byte, 16 client : 15.7k Q/s (nodelay)
512byte, 16 client : 18.9k Q/s (delayed)
1024byte, 16 client : 10.2k Q/s  (nodelay) ここから急激に落ちる(Etherの都合)
1100byte, 16 client : 9.2k Q/s (nodelay)
1200byte, 16 client : 8.3k Q/s (nodelay)
1400byte, 16 client : 7.2k Q/s (nodelay)
1500byte, 16 client : 6.6k Q/s (nodelay)
1700byte, 16 client : 1.4k Q/s (nodelay)
2048byte, 16 client : 2.2k Q/s (nodelay)
3000byte, 16 client : 1.6k Q/s (nodelay)
3000byte, 16 client : 2.0k Q/s (delayed)
</Pre>
<p>
上の例ではいずれの場合も、サーバプロセスは最高速度が出ている時点で、
CPU 使用率はほぼ 100% となり、その時間のほとんどは I/O 待ちである。
I/O を待っている間もカーネル(デバイスドライバ)は仕事をしているので、
CPU としては、他のことに使える時間はほとんどない。
</p>
<p>
結局、暗号なしで中継も使わない場合は、
1クライアント、1バイトクエリ での 50.1k Q/s という速度が、
100Mbps環境での限界となる。
もちろんこの結果はNIC の影響で多少上下する。
それは今後のすべてのテストで同様である。
</p>

<h3>暗号を使う場合</h3>
<p>
Blowfish/64bit の暗号化を使うと、コネクション数にかかわらず、
P3 933MHz では、以下のような Q/s が出た。
Q/s は大幅に減っているが、
この場合は、CPU の時間のほとんどは暗号の処理に使われている。
ほかの事に使える時間はほぼゼロである。
たとえば 16byte query を使う場合は、 3.0k Q/s の利用率にすれば、
CPU は50% 空くという事である。
</p>
<pre>
1byte,  10 client : 9.4k Q/s (delayed)
1byte,  256 client : 7.7k Q/s (delayed)
16byte,  10 client : 7.6k Q/s (delayed)
16byte,  256 client : 6.0k Q/s (delayed)
256byte,  10 client : 2.4k Q/s (delayed)
256byte,  256 client : 2.1k Q/s (delayed)
1024byte,  10 client : 741 Q/s (delayed)
2048byte,  10 client : 378 Q/s (delayed)
</pre>
<p>
この結果を見ると、 P3 933MHz マシンは、概算で
<pre>
2048 * 378 * 8bit * (全2重) = 12.3 Mbps
</pre>
 の Blowfish 暗号化能力があることがわかる。
</p>


<H3>中継を使って高速化するが、暗号は使わない場合</h3>
<p>
  暗号を使わない場合でも中継を使うと高速化されるかどうか、
計測してみた。中継サーバと本サーバの間のプロトコルヘッダは
TCP のヘッダの半分もないので、
先の 50.1k Q/s を越えられる可能性が高い。
ここでは、中継サーバと本サーバの間のトラフィックが、
クライアントと中継サーバのトラフィックと干渉しないようにネットワークを
分離して実験した。
最初の実験結果により、Proxy が受けとれる Q/s は最高でも 50.1k Q/s なので、
まずその値が出るか調べた。
その結果、 69.5k Q/s という値が出た。
50.1k Q/s より速い理由は、カーネルの TCP/IP ソケットハッシュより、
VCE のクライアント検索エンジンのほうがアルゴリズムが単純
(その分メモリを食っているとも言う)な分速い、という事のようだ。
</p>
<p>
さらに、Proxy マシン上でもクライアントを動かしてみた。
その結果 106.9k Q/s という値が出た。
この値が 100Mbps での 1バイトクエリの限界速度に近いはずである。
</p>

<pre>

        192.168.0.0/24            192.168.1.0/24

Server ----- Switch ----- Proxy ---- Switch ---- Client
P3 933                    P3 1G          |       P3 600*2
                                         +------ Client
                                                 P3 550
</pre>
<pre>
1byte, 1 client : 2.5k Q/s
1byte,  60 client : 69.5k Q/s
1byte,  128 client : 106.9k Q/s
16byte,  128 client : 83.0k Q/s
64byte,  128 client : 58.5k Q/s
128byte,  128 client : 30.5k Q/s
256byte,  1 client : 12.8k Q/s
256byte,  4 client : 15.5k Q/s
256byte,  16 client : 18.0k Q/s
1024byte, 8 client : 7.7 Q/s
</pre>
<p>
  256バイトを越え、パケットが大きくなってくると、
中継を使わずに直接やりとりするよりも Q/s は低くなる。
これは中継サーバのヘッダによるものである。
ただし、この場合でもServer の CPU 利用率は 10%以下であることが
「非暗号化、中継なし」の場合と異なる。
これは VCE のクライアント検索ルーチンで処理している事、
大きな単位で I/O している事、などが理由である。
</p>

<h3>中継も暗号化も使う場合</h3>
<p>
  では最後に、クライアントと中継の間の通信を暗号化する場合をテストする。
これは中継サーバが最も本領を発揮するパターンである。
これまでの実験で得られた値結果により、
1個の中継は大体 9k Q/s のプロクシ能力を持つと考えられる。
上の実験と同じネットワーク環境で Proxy - Client 間の通信を
Blowfish/8 で暗号化してみた。
その結果、
</p>

<pre>
1byte, 10 client, 1 swp : 9.3k Q/s
1byte, 128 client, 1 swp : 9.5k Q/s
2048byte, 10 client, 1 swp : 380 Q/s
</pre>
<p>
  という値が出たので、ほぼ理論通りとなっていることがわかる。
この段階では Server の NIC はまだまだ余裕があるので、
Server が走っているマシン(P3 933MHzが1個余っている)
で中継サーバをもうひとつ立ちあげてlocalhost の Server につなぎ、
素直に Q/s が増加するか見る。
そうすると、ほぼ2倍の、 19.1k Q/s を達成できた。
</p>

<pre>
1byte, 20 client, 2 swp : 19.1k Q/s
2048byte, 20 client, 2 swp : 842 Q/s
2048byte, 30 client, 3 swp : 1260 Q/s
</pre>

<p>
swp の台数を増やしていくと、線形に Q/s が増加しているのがわかる。
当然、この時点では、 Server の CPU 使用率は 1% 未満であり、
暗号の処理が完全に外部化されている事がわかる。
もっと CPU(マシン) があれば Server 側の限界を検証できるのだが、
現在はマシンが足りない。
おそらく Server の NIC をフル稼働させるほどの Q/s を実現するには
あと5台程度の swp マシンがあればよいだろう。
</p>



<h3>1000Base でのテスト</h3>
<p>
  100Base での検証は大体終わったので、次は 1000Base でのテストをしたいが、
これはコミュニティーエンジンにテスト環境ができてからおこなう。
</p>

<h3>まとめ</h3>
<p>
   swp の処理性能が P3 1GHz で 12.3Mbps であること、
暗号を使う場合は確実に swp を使うべきであること、
暗号を使わずに大きなパケットをやりとりする場合は swp の効果は
それほど劇的でないことなどが実験によって明らかとなった。
また、 VCE を使ってアプリケーション開発をするときには、
そのアプリケーションが、どの程度の Q/s を必要としているかをまず考え、
それに応じて swp の形態などをデザインしていく必要がある。
</p>

<br><br>
<a href="index.html">index に戻る</a>
<br>

<div id="footer">#COPYRIGHT#</div>

</body> </html>
